{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2b5_3.ipynb","provenance":[],"collapsed_sections":["-WEDZ53Zbu0h","VcDJZphiU4ec"],"authorship_tag":"ABX9TyMj2ccym/KP+d6Axp/JjBOB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-WEDZ53Zbu0h"},"source":["# Question 2b5_3\n","Compare the test accuracies and the running times of the networks implemented in parts (1) – (4).\n","\n","Experiment with adding dropout to the layers of networks in parts (1) – (4), and report the test accuracies. Compare and comment on the accuracies of the networks with/without dropout."]},{"cell_type":"markdown","metadata":{"id":"L5pxx5f-Uskg"},"source":["# Imports and Setup"]},{"cell_type":"code","metadata":{"id":"HdVcgqJuT-r1","executionInfo":{"status":"ok","timestamp":1605190680080,"user_tz":-480,"elapsed":17322,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"a01fd415-b073-4850-dfe4-e21c9555127f","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# chuanxin\n","%cd \"../gdrive/My Drive/cz4042_assignment_2/2b\" "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/cz4042_assignment_2/2b\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYLMIZfUUu19"},"source":["import os\n","import time\n","import json\n","import csv\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWdh-qfGWLb1"},"source":["seed = 10\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcDJZphiU4ec"},"source":["# Helper functions"]},{"cell_type":"markdown","metadata":{"id":"upXJ-u4cU8gY"},"source":["### read_data_chars()\n","Used to load in the data. Returns x_train, y_train, x_test, y_test"]},{"cell_type":"code","metadata":{"id":"cBuzMdbSU3em"},"source":["def read_data_chars():\n","    x_train, y_train, x_test, y_test = [], [], [], []\n","    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n","    with open('./train_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_train.append(data)\n","            y_train.append(int(row[0]))\n","\n","    with open('./test_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_test.append(data)\n","            y_test.append(int(row[0]))\n","\n","\n","    vocab_size, char_to_ix = vocabulary(x_train+x_test)\n","    x_train = preprocess(x_train, char_to_ix, MAX_DOCUMENT_LENGTH)\n","    y_train = np.array(y_train)\n","    x_test = preprocess(x_test, char_to_ix, MAX_DOCUMENT_LENGTH)\n","    y_test = np.array(y_test)\n","\n","    x_train = tf.constant(x_train, dtype=tf.int64)\n","    y_train = tf.constant(y_train, dtype=tf.int64)\n","    x_test = tf.constant(x_test, dtype=tf.int64)\n","    y_test = tf.constant(y_test, dtype=tf.int64)\n","\n","    return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lw4xNc4tb9mh"},"source":["### vocabulary(strings)\n","Read data with [character]. Get the unique characters in this strings and the vocab size (of characters)"]},{"cell_type":"code","metadata":{"id":"2x7REe4-cBQA"},"source":["def vocabulary(strings):\n","    chars = sorted(list(set(list(''.join(strings)))))\n","    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","    vocab_size = len(chars)\n","    return vocab_size, char_to_ix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af_uf97ecZ8M"},"source":["### preprocess(strings, char_to_ix, MAX_LENGTH)\n","Clean up a string "]},{"cell_type":"code","metadata":{"id":"E1gJ4CxYcZp5"},"source":["def preprocess(strings, char_to_ix, MAX_LENGTH):\n","    data_chars = [list(d.lower()) for _, d in enumerate(strings)]\n","    for i, d in enumerate(data_chars):\n","        if len(d)>MAX_LENGTH:\n","            d = d[:MAX_LENGTH]\n","        elif len(d) < MAX_LENGTH:\n","            d += [' '] * (MAX_LENGTH - len(d))\n","            \n","    data_ids = np.zeros([len(data_chars), MAX_LENGTH], dtype=np.int64)\n","    for i in range(len(data_chars)):\n","        for j in range(MAX_LENGTH):\n","            data_ids[i, j] = char_to_ix[data_chars[i][j]]\n","    return np.array(data_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7NvMFU2KZCc"},"source":["### make_directories()\n","Used to create directories that might not have been made"]},{"cell_type":"code","metadata":{"id":"xl440MwmKY4C"},"source":["# Create folder to store histories and figures\n","def make_directories():\n","  if not os.path.exists('./histories'):\n","    os.mkdir('./histories')\n","  if not os.path.exists('./figures'):\n","    os.mkdir('./figures')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAbBYp9iKJ6R"},"source":["### history_saver(history, filename, already_json=False)\n","Used to save a history object"]},{"cell_type":"code","metadata":{"id":"Hp6cbjJhKJnS"},"source":["# filename like 'history/model_name.json'\n","def history_saver(history, model_name, already_json=False):\n","  history_json = {}\n","\n","  if already_json:\n","    history_json = history\n","  else:\n","    history = history.history\n","    for key in history.keys():\n","      history_json[key] = history[key]\n","\n","  with open('./histories/' + model_name, 'w') as file:\n","    json.dump(history_json, file)\n","\n","  print(\"History saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zCk874hLSKK"},"source":["### history_loader(filename)\n","Used to load in a json history object"]},{"cell_type":"code","metadata":{"id":"yqCAHrE4LShy"},"source":["# filename like 'history/model_name.json'\n","def history_loader(model_name):\n","  with open('./histories/'+model_name) as json_file:\n","    history = json.load(json_file)\n","  print('History loaded')\n","  \n","  return history "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_URv4UfQoGr"},"source":["### plot_loss(history_json, model_name)\n","Plot out loss graph, and also save it"]},{"cell_type":"code","metadata":{"id":"9ERB0N0cQodE"},"source":["def plot_loss(history_json, model_name):\n","  train_loss = history_json['loss']\n","  test_loss = history_json['test_loss']\n","  title = 'Model name: ' + model_name + '\\nloss against epochs'\n","\n","  plt.plot(train_loss, label='train')\n","  plt.plot(test_loss, label='test')\n","  plt.title(title)\n","  plt.ylabel('loss')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_loss.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69De_d4XQuBZ"},"source":["### plot_acc(history_json, model_name)\n","Plot out accuracy graph, and also save it"]},{"cell_type":"code","metadata":{"id":"lU9d-fF5QuW1"},"source":["def plot_acc(history_json, model_name):\n","  train_acc = history_json['accuracy']\n","  test_acc = history_json['test_accuracy']\n","  title = 'Model name: ' + model_name + '\\naccuracy against epochs'\n","\n","  plt.plot(train_acc, label='train')\n","  plt.plot(test_acc, label='test')\n","  plt.title(title)\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_accuracy.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDvlc9ZbWhZl"},"source":["# 2b5_3 - CharRNN (GRU) (dropout)"]},{"cell_type":"code","metadata":{"id":"z1U1trQWVDuG"},"source":["tf.keras.backend.set_floatx('float32')\n","class CharRNN(tf.keras.Model):\n","  def __init__(self, vocab_size, use_dropout, hidden_dims, rnn_type, num_rnn_layers):\n","    super(CharRNN, self).__init__()\n","    self.vocab_size = vocab_size\n","    self.use_dropout = use_dropout\n","\n","    # Weight variables and RNN cell\n","    self.hidden_dims = hidden_dims\n","    self.rnn_type = rnn_type\n","    self.num_rnn_layers = num_rnn_layers\n","\n","    if self.rnn_type == 'GRU' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'GRU' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'SimpleRNN' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True)     \n","    elif self.rnn_type == 'SimpleRNN' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True)     \n","    elif self.rnn_type == 'LSTM' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'LSTM' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True)\n","    else:\n","      print(\"wrong input\")   \n","\n","    self.dense = tf.keras.layers.Dense(MAX_LABEL, activation=None)\n","\n","  def call(self, x, drop_rate):\n","    # forward\n","    x = tf.one_hot(x, one_hot_size)\n","    encoding = self.rnn1(x)\n","    if self.num_rnn_layers == 2:\n","      encoding = self.rnn2(encoding)\n","    if self.use_dropout:\n","      encoding = tf.nn.dropout(encoding, drop_rate)\n","    logits = self.dense(encoding)\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOf6Fd5rueTA"},"source":["# Training function\n","def train_step(model, x, label, optimizer, drop_rate):\n","  with tf.GradientTape() as tape:\n","    out = model(x, drop_rate)\n","    loss = loss_object(label, out)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","      \n","  train_loss(loss)\n","  train_accuracy(label, out)\n","\n","# Testing function\n","def test_step(model, x, label, drop_rate):\n","  out = model(x,drop_rate)\n","  t_loss = loss_object(label, out)\n","  test_loss(t_loss)\n","  test_accuracy(label, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9tl4IUM5uhlx"},"source":["def fit(model):\n","  train_acc_list, train_loss_list, test_acc_list, test_loss_list = [], [], [], []\n","  time_start = time.perf_counter()\n","\n","  for epoch in range(no_epochs):\n","    # Reset the metrics at the start of the next epoch\n","    train_accuracy.reset_states()\n","    train_loss.reset_states()\n","    test_accuracy.reset_states()\n","    test_loss.reset_states()\n","\n","    for images, labels in train_ds:\n","      train_step(model, images, labels, optimizer, 0.5)\n","\n","    for images, labels in test_ds:\n","      test_step(model, images, labels, 0)\n","\n","    \n","    train_acc_list.append(train_accuracy.result())\n","    train_loss_list.append(train_loss.result())\n","    test_acc_list.append(test_accuracy.result())\n","    test_loss_list.append(test_loss.result())\n","\n","    template = 'Epoch {}, train_accuracy: {}, train_loss: {}, test_accuracy: {}, test_loss: {}'\n","    print (template.format(epoch+1,\n","                          train_accuracy.result(),\n","                          train_loss.result(),\n","                          test_accuracy.result(),\n","                          test_loss.result()))\n","\n","  time_stop = time.perf_counter()\n","  time_taken = time_stop-time_start\n","\n","  return train_acc_list, train_loss_list, test_acc_list, test_loss_list, time_taken"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCH46KpPx1Wu"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"9lUtvHlHuUTT"},"source":["MAX_DOCUMENT_LENGTH = 100\n","HIDDEN_SIZE = 20\n","RNN_TYPE = 'GRU'\n","NUM_RNN_LAYERS = 1\n","MAX_LABEL = 15\n","\n","batch_size = 128\n","one_hot_size = 256\n","no_epochs = 250\n","lr = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9LTnMMI1IUd"},"source":["# Choose optimizer and loss function for training\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n","\n","# Select metrics to measure the loss and the accuracy of the model. \n","# These metrics accumulate the values over epochs and then print the overall result.\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW3ZGh2c2wyZ"},"source":["# Training and test\n","make_directories()\n","x_train, y_train, x_test, y_test = read_data_chars()\n","\n","# Use `tf.data` to batch and shuffle the dataset:\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEagapWE0uz_","outputId":"bd1ba2b8-8ff8-4ef8-9941-3724416b3e6f","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = CharRNN(256, True, HIDDEN_SIZE, RNN_TYPE, NUM_RNN_LAYERS)\n","model_name = 'q5_3_char_rnn_gru_dropout'\n","train_acc_list, train_loss_list, test_acc_list, test_loss_list, time_taken = fit(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1, train_accuracy: 0.08749999850988388, train_loss: 2.6424436569213867, test_accuracy: 0.11571428924798965, test_loss: 2.599637746810913\n","Epoch 2, train_accuracy: 0.11821428686380386, train_loss: 2.582338571548462, test_accuracy: 0.11428571492433548, test_loss: 2.5360212326049805\n","Epoch 3, train_accuracy: 0.18535713851451874, train_loss: 2.348172187805176, test_accuracy: 0.20571428537368774, test_loss: 2.285818338394165\n","Epoch 4, train_accuracy: 0.2596428692340851, train_loss: 2.096890449523926, test_accuracy: 0.2914285659790039, test_loss: 2.05159592628479\n","Epoch 5, train_accuracy: 0.33250001072883606, train_loss: 1.8945140838623047, test_accuracy: 0.36142855882644653, test_loss: 1.8233165740966797\n","Epoch 6, train_accuracy: 0.4348214268684387, train_loss: 1.641984462738037, test_accuracy: 0.4528571367263794, test_loss: 1.5899128913879395\n","Epoch 7, train_accuracy: 0.4982142746448517, train_loss: 1.4462512731552124, test_accuracy: 0.5042856931686401, test_loss: 1.4201184511184692\n","Epoch 8, train_accuracy: 0.550000011920929, train_loss: 1.3080331087112427, test_accuracy: 0.5185714364051819, test_loss: 1.3570693731307983\n","Epoch 9, train_accuracy: 0.5883928537368774, train_loss: 1.1947623491287231, test_accuracy: 0.5671428442001343, test_loss: 1.2892009019851685\n","Epoch 10, train_accuracy: 0.6119642853736877, train_loss: 1.1352936029434204, test_accuracy: 0.5785714387893677, test_loss: 1.2515597343444824\n","Epoch 11, train_accuracy: 0.6401785612106323, train_loss: 1.0581926107406616, test_accuracy: 0.6000000238418579, test_loss: 1.1995314359664917\n","Epoch 12, train_accuracy: 0.6625000238418579, train_loss: 0.9904470443725586, test_accuracy: 0.6171428561210632, test_loss: 1.1591134071350098\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YXPSuKJl5ZAt"},"source":["for metric_list in [train_acc_list, train_loss_list, test_acc_list, test_loss_list]:\n","  metric_list[:] = [x.numpy() for x in metric_list]\n","  metric_list[:] = [x.astype(float) for x in metric_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLPeXFWA10P6"},"source":["history_json = {model_name: {\n","    'accuracy': train_acc_list,\n","    'test_accuracy': test_acc_list,\n","    'loss': train_loss_list,\n","    'test_loss': test_loss_list,\n","    'time_taken': time_taken\n","}}\n","\n","history_saver(history_json, model_name, already_json=True)\n","histories_json = history_loader(model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUr2r3jh11Ak"},"source":["plot_acc(histories_json[model_name], model_name)\n","plot_loss(histories_json[model_name], model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogOPrzW4VgGb"},"source":["histories_json[model_name]['time_taken']"],"execution_count":null,"outputs":[]}]}