{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2a2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmy+O+R4C4HXnf+GeU7/gN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-WEDZ53Zbu0h"},"source":["# Question 2a2\n","Use a grid search ( c1 in {10, 30, 50, 70, 90}, c2 in {20, 40, 60, 80, 100} , in total 25 combinations) to find the optimal combination of the numbers of channels at the convolution layers. Use the test accuracy to determine the optimal combination. Report all 25 accuracies."]},{"cell_type":"markdown","metadata":{"id":"L5pxx5f-Uskg"},"source":["# Imports and Setup"]},{"cell_type":"code","metadata":{"id":"HdVcgqJuT-r1","executionInfo":{"status":"ok","timestamp":1605122385237,"user_tz":-480,"elapsed":37897,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"0ebbdfa9-d451-4b4c-d8de-5f94b7225474","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# chuanxin\n","%cd \"../gdrive/My Drive/cz4042_assignment_2/2a\" "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/cz4042_assignment_2/2a\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYLMIZfUUu19"},"source":["import os\n","import pickle\n","import json\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import tensorflow as tf\n","from tqdm.keras import TqdmCallback"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWdh-qfGWLb1"},"source":["seed = 0\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","inspect_index = 40"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcDJZphiU4ec"},"source":["# Helper functions"]},{"cell_type":"markdown","metadata":{"id":"upXJ-u4cU8gY"},"source":["### load_data(file)\n","Used to load in the data "]},{"cell_type":"code","metadata":{"id":"cBuzMdbSU3em"},"source":["# Fixed, no need change\n","def load_data(file):\n","  with open(file, 'rb') as fo:\n","    try:\n","      samples = pickle.load(fo)\n","    except UnicodeDecodeError:  # python 3.x\n","      fo.seek(0)\n","      samples = pickle.load(fo, encoding='latin1')\n","\n","  data, labels = samples['data'], samples['labels']\n","\n","  data = np.array(data, dtype=np.float32) / 255\n","  labels = np.array(labels, dtype=np.int32)\n","  \n","  data = data.reshape(len(data), 3, 32, 32).transpose(0, 2, 3, 1)\n","  return data, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7NvMFU2KZCc"},"source":["### make_directories()\n","Used to create directories that might not have been made"]},{"cell_type":"code","metadata":{"id":"xl440MwmKY4C"},"source":["# Create folder to store histories and figures\n","def make_directories():\n","  if not os.path.exists('./histories'):\n","      os.mkdir('./histories')\n","  if not os.path.exists('./figures'):\n","      os.mkdir('./figures')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAbBYp9iKJ6R"},"source":["### history_saver(history, filename, already_json=False)\n","Used to save a history object"]},{"cell_type":"code","metadata":{"id":"Hp6cbjJhKJnS"},"source":["# filename like 'history/model_name.json'\n","def history_saver(history, model_name, already_json=False):\n","  history_json = {}\n","\n","  if already_json:\n","    history_json = history\n","  else:\n","    history = history.history\n","    for key in history.keys():\n","      history_json[key] = history[key]\n","\n","  with open('./histories/' + model_name, 'w') as file:\n","    json.dump(history_json, file)\n","\n","  print(\"History saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zCk874hLSKK"},"source":["### history_loader(filename)\n","Used to load in a json history object"]},{"cell_type":"code","metadata":{"id":"yqCAHrE4LShy"},"source":["# filename like 'history/model_name.json'\n","def history_loader(model_name):\n","  with open('./histories/'+model_name) as json_file:\n","    history = json.load(json_file)\n","  print('History loaded')\n","  \n","  return history "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmfPiQevbXu4"},"source":["### plot_3_channel_img(np_array):"]},{"cell_type":"code","metadata":{"id":"teGJdbZ5XDwc"},"source":["def plot_3_channel_img(np_array, inspect_index=None, filename=None):\n","  if inspect_index == None:\n","    inspect_index = np.random.randint(len(np_array))\n","\n","  titles = ['Original', 'Red channel', 'Green channel', 'Blue channel']\n","  cmaps = [None, plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n","\n","  fig, axes = plt.subplots(1, 4, figsize=(32,8))\n","  objs = zip(axes, (np_array[inspect_index], *np_array[inspect_index].transpose(2,0,1)), titles, cmaps)\n","\n","  if filename == None:\n","    print(\"index\", inspect_index)\n","\n","  for ax, channel, title, cmap in objs:\n","    ax.imshow(channel, cmap=cmap)\n","    ax.set_title(title)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","  \n","  if filename != None:\n","    fig.savefig(f'./figures/{filename}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_URv4UfQoGr"},"source":["### plot_loss(history_json, model_name)\n","Plot out loss graph, and also save it"]},{"cell_type":"code","metadata":{"id":"9ERB0N0cQodE"},"source":["def plot_loss(history_json, model_name):\n","  train_acc = history_json['loss']\n","  test_acc = history_json['val_loss']\n","  title = 'Model name: ' + model_name + '\\nloss against epochs'\n","\n","  plt.plot(train_acc, label='train')\n","  plt.plot(test_acc, label='test')\n","  plt.title(title)\n","  plt.ylabel('loss')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_loss.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69De_d4XQuBZ"},"source":["### plot_acc(history_json, model_name)\n","Plot out accuracy graph, and also save it"]},{"cell_type":"code","metadata":{"id":"lU9d-fF5QuW1"},"source":["def plot_acc(history_json, model_name):\n","  train_acc = history_json['accuracy']\n","  test_acc = history_json['val_accuracy']\n","  title = 'Model name: ' + model_name + '\\naccuracy against epochs'\n","\n","  plt.plot(train_acc, label='train')\n","  plt.plot(test_acc, label='test')\n","  plt.title(title)\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_accuracy.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOAkSmZ7lpON"},"source":["### plot_activations(model, model_name, x_data, x_data_name)\n","Get the intermediate activations for a single x_data (one item in x_train or x_test)"]},{"cell_type":"code","metadata":{"id":"RfcKr-54lph1"},"source":["def plot_activations(model, model_name, x_data, x_data_name):\n","  plot_3_channel_img(x_data, filename=f'{model_name}_{x_data_name}_source.png')\n","\n","  layer_outputs = [layer.output for layer in model.layers]\n","  activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n","  activations = activation_model.predict(x_data) \n","\n","  activation_layer_names = ['conv1', 'max1', 'conv2', 'max2']  \n","  activations = activations[:4]\n","\n","  images_per_row = 10\n","  \n","  for layer_name, layer_activation in zip(activation_layer_names, activations): # Displays the feature maps\n","    n_features = layer_activation.shape[-1] # Number of features in the feature map\n","    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n","    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n","    display_grid = np.zeros((size * n_cols, images_per_row * size))\n","\n","    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n","      for row in range(images_per_row):\n","        channel_image = layer_activation[0, :, :, col * images_per_row + row]\n","        channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n","        channel_image /= channel_image.std()\n","        channel_image *= 64\n","        channel_image += 128\n","        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n","        display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image # Displays the grid \n","        \n","    scale = 1. / size\n","    plt.figure(figsize=(scale * display_grid.shape[1],\n","                        scale * display_grid.shape[0]))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.axis('off')\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n","    plt.savefig(f'./figures/{model_name}_{x_data_name}_{layer_name}.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDvlc9ZbWhZl"},"source":["# 2a2 - grid search \n","\n","c1 in {10, 30, 50, 70, 90}\n","\n","c2 in {20, 40, 60, 80, 100}"]},{"cell_type":"code","metadata":{"id":"aeU94onuWg1s"},"source":["# Training and test\n","make_directories()\n","x_train, y_train = load_data('data_batch_1')\n","x_test, y_test = load_data('test_batch_trim')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVwY7mMpVEOd"},"source":["### make_model(num_ch_c1, num_ch_c2, use_dropout)\n","Creates a CNN model that has number of channels for the Conv2D layer specified, and also dropout, if needed"]},{"cell_type":"code","metadata":{"id":"z1U1trQWVDuG"},"source":["def make_model(num_ch_c1, num_ch_c2, use_dropout):\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Input(shape=(32, 32, 3), name='input'))\n","    model.add(tf.keras.layers.Conv2D(num_ch_c1, 9, activation='relu', input_shape=(None, None, 3), padding='valid', name='conv1'))\n","    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid', name='max1'))\n","    model.add(tf.keras.layers.Conv2D(num_ch_c2, 5, activation='relu', input_shape=(None, None, 3), padding='valid', name='conv2'))\n","    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid', name='max2'))\n","    model.add(tf.keras.layers.Flatten(name='flatten'))\n","    model.add(tf.keras.layers.Dense(300, use_bias=True, name='dense1'))\n","    if use_dropout:\n","      model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout'))\n","    model.add(tf.keras.layers.Dense(10, use_bias=True, input_shape=(300,), name='dense2'))  # Here no softmax because we have combined it with the loss    \n","    # model.add(tf.keras.layers.Softmax(axis=-1, name='dense2'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0M7WCSjyCnAG"},"source":["def set_model_parameters(optimizer_, model_name):\n","  # fixed \n","  epochs = 1000  \n","  batch_size = 128  \n","  learning_rate = 0.001 \n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  callbacks = [TqdmCallback(verbose=1)]\n","\n","\n","  if optimizer_ == 'SGD':\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","  elif optimizer_ == 'SGD-momentum':\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.1)\n","  elif optimizer_ == 'RMSProp': \n","    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","  elif optimizer_ == 'Adam': \n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  else:\n","    raise NotImplementedError(f'You do not need to handle [{optimizer_}] in this project.')\n","\n","  return epochs, batch_size, loss, callbacks, optimizer, model_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxLNUD8msgFh"},"source":["num_ch_c1_list = [10,30,50,70,90]\n","num_ch_c2_list = [20,40,60,80,100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JigygwFQGW47"},"source":["for num_ch_c1 in num_ch_c1_list:\n","  for num_ch_c2 in num_ch_c2_list:\n","\n","    model = make_model(num_ch_c1, num_ch_c2, False)\n","    epochs, batch_size, loss, callbacks, optimizer, model_name = set_model_parameters('SGD', f'q2_model_{num_ch_c1}_{num_ch_c2}')\n","\n","    model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n","\n","    history = model.fit(\n","        x_train,\n","        y_train,\n","        batch_size=batch_size,\n","        epochs=epochs,\n","        callbacks=callbacks,\n","        validation_data=(x_test, y_test))\n","\n","    history_saver(history, model_name)\n","    history_json = history_loader(model_name)\n","\n","    plot_acc(history_json, model_name)\n","    plot_loss(history_json, model_name)\n","\n","    for i in range(2):\n","      print(\"-\"*100)\n","      print(\"x_test\", i)\n","      x_data = np.expand_dims(x_test[i], axis=0) \n","      plot_activations(model, model_name, x_data[:2], 'xtest' + str(i))\n","      print(\"-\"*100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JlCkdTdpfPS"},"source":["# Conclusion"]},{"cell_type":"code","metadata":{"id":"v40SKovcdYUJ"},"source":[""],"execution_count":null,"outputs":[]}]}