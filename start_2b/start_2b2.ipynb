{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"start_2b2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/dgkbYf9RJpOqjg0HxIPw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"RgHk49aeIwtf","executionInfo":{"status":"ok","timestamp":1604930225543,"user_tz":-480,"elapsed":2119,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"5aed68a2-cc73-4ef8-9838-8fadee2d87a3","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# chuanxin\n","%cd \"../gdrive/My Drive/cz4042_assignment_2/start_2b\" "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/cz4042_assignment_2/start_2b\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"galBjK1tGD-5","executionInfo":{"status":"ok","timestamp":1604932202079,"user_tz":-480,"elapsed":1125,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"a48ec217-5448-4149-eb68-f53a0e21666d","colab":{"base_uri":"https://localhost:8080/"}},"source":["import os\n","import numpy as np\n","import collections\n","import tensorflow as tf\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from tensorflow.keras import Model, layers\n","import csv\n","import re\n","import pylab"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GDRhriMFOXaA"},"source":["MAX_DOCUMENT_LENGTH = 100\n","N_FILTERS = 10\n","HIDDEN_SIZE = 20\n","EMBEDDING_SIZE = 50\n","FILTER_SHAPE1 = [20, 256]\n","POOLING_WINDOW = 4\n","POOLING_STRIDE = 2\n","MAX_LABEL = 15\n","\n","batch_size = 128\n","no_epochs = 1 # 100\n","lr = 0.01\n","\n","seed = 10\n","tf.random.set_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqpCMY_FOcRB"},"source":["def clean_str(text):\n","    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    text = text.strip().lower()\n","\n","    return text\n","\n","\n","def build_word_dict(contents):\n","    words = list()\n","    for content in contents:\n","        for word in word_tokenize(clean_str(content)):\n","            words.append(word)\n","\n","    word_counter = collections.Counter(words).most_common()\n","    word_dict = dict()\n","    word_dict[\"<pad>\"] = 0\n","    word_dict[\"<unk>\"] = 1\n","    word_dict[\"<eos>\"] = 2\n","    for word, _ in word_counter:\n","        word_dict[word] = len(word_dict)\n","    return word_dict\n","\n","\n","def preprocess(contents, word_dict, document_max_len):\n","    x = list(map(lambda d: word_tokenize(clean_str(d)), contents))\n","    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n","    x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n","    x = list(map(lambda d: d[:document_max_len], x))\n","    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n","    return x\n","\n","\n","def read_data_words():\n","    x_train, y_train, x_test, y_test = [], [], [], []\n","    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n","    with open('./train_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_train.append(data)\n","            y_train.append(int(row[0]))\n","\n","    with open('./test_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_test.append(data)\n","            y_test.append(int(row[0]))\n","\n","    word_dict = build_word_dict(x_train+x_test)\n","    x_train = preprocess(x_train, word_dict, MAX_DOCUMENT_LENGTH)\n","    y_train = np.array(y_train)\n","    x_test = preprocess(x_test, word_dict, MAX_DOCUMENT_LENGTH)\n","    y_test = np.array(y_test)\n","\n","    x_train = [x[:MAX_DOCUMENT_LENGTH] for x in x_train]\n","    x_test = [x[:MAX_DOCUMENT_LENGTH] for x in x_test]\n","    x_train = tf.constant(x_train, dtype=tf.int64)\n","    y_train = tf.constant(y_train, dtype=tf.int64)\n","    x_test = tf.constant(x_test, dtype=tf.int64)\n","    y_test = tf.constant(y_test, dtype=tf.int64)\n","\n","    vocab_size = tf.get_static_value(tf.reduce_max(x_train))\n","    vocab_size = max(vocab_size, tf.get_static_value(tf.reduce_max(x_test))) + 1\n","    return x_train, y_train, x_test, y_test, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0QvWDb-OhTH"},"source":["x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n","# Use `tf.data` to batch and shuffle the dataset:\n","train_ds = tf.data.Dataset.from_tensor_slices(\n","    (x_train, y_train)).shuffle(10000).batch(batch_size)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","# Build model\n","tf.keras.backend.set_floatx('float32')\n","class WordRNN(Model):\n","\n","    def __init__(self, vocab_size, hidden_dim=10):\n","        super(WordRNN, self).__init__()\n","        # Hyperparameters\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.embedding = layers.Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_DOCUMENT_LENGTH)\n","        # Weight variables and RNN cell\n","        self.rnn = layers.RNN(\n","            tf.keras.layers.GRUCell(self.hidden_dim), unroll=True)\n","        self.dense = layers.Dense(MAX_LABEL, activation=None)\n","\n","    def __call__(self, x, drop_rate):\n","        # forward logic\n","        embedding = self.embedding(x)\n","        encoding = self.rnn(embedding)\n","\n","        encoding = tf.nn.dropout(encoding, drop_rate)\n","        logits = self.dense(encoding)\n","    \n","        return logits\n","\n","model = WordRNN(vocab_size, HIDDEN_SIZE)\n","\n","# Choose optimizer and loss function for training\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Select metrics to measure the loss and the accuracy of the model. \n","# These metrics accumulate the values over epochs and then print the overall result.\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","# Training function\n","def train_step(model, x, label, drop_rate):\n","    with tf.GradientTape() as tape:\n","        out = model(x, drop_rate)\n","        loss = loss_object(label, out)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        \n","    train_loss(loss)\n","    train_accuracy(labels, out)\n","\n","# Testing function\n","def test_step(model, x, label, drop_rate=0):\n","    out = model(x,drop_rate)\n","    t_loss = loss_object(label, out)\n","    test_loss(t_loss)\n","    test_accuracy(label, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQCCl8RHOmC6","executionInfo":{"status":"error","timestamp":1604932222839,"user_tz":-480,"elapsed":21861,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"8cc981d4-3c5e-4bcf-fd8d-348de5690778","colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["test_acc = []\n","for epoch in range(no_epochs):\n","    # Reset the metrics at the start of the next epoch\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    test_loss.reset_states()\n","    test_accuracy.reset_states()\n","\n","    for images, labels in train_ds:\n","        train_step(model, images, labels, drop_rate=0.5)\n","\n","    for images, labels in test_ds:\n","        test_step(model, images, labels, drop_rate=0)\n","\n","    test_acc.append(test_accuracy.result())\n","    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n","    print (template.format(epoch+1,\n","                          train_loss.result(),\n","                          train_accuracy.result(),\n","                          test_loss.result(),\n","                          test_accuracy.result()))\n","    \n","\n","# Create folder to store models and results\n","if not os.path.exists('./models'):\n","    os.mkdir('./models')\n","if not os.path.exists('./results'):\n","    os.mkdir('./results')\n","\n","# model.save(f'./models/start_2b2')    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1, Loss: 2.6802196502685547, Accuracy: 0.06803571432828903, Test Loss: 2.6564505100250244, Test Accuracy: 0.0714285746216774\n","WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.WordRNN object at 0x7f46678d2080>, because it is not built.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-355d02632ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./models/start_2b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m--> 134\u001b[0;31m                           signatures, options)\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_skip_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_model_input_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mraise_model_input_error\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0;34m'set. Usually, input shapes are automatically determined from calling'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;34m' `.fit()` or `.predict()`. To manually set the shapes, call '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       '`model.build(input_shape)`.'.format(model))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Model <__main__.WordRNN object at 0x7f46678d2080> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling `.fit()` or `.predict()`. To manually set the shapes, call `model.build(input_shape)`."]}]},{"cell_type":"code","metadata":{"id":"gbItTEBBKQw-"},"source":["# Plot test accuracy\n","pylab.figure()\n","pylab.plot(np.arange(no_epochs), test_acc)\n","pylab.xlabel('epochs')\n","pylab.ylabel('test accuracy')\n","pylab.legend(loc='lower right')\n","\n","\n","pylab.savefig(\n","    f'./results/start_2b2_accuracy.pdf'\n",")"],"execution_count":null,"outputs":[]}]}