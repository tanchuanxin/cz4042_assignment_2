{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2b0.ipynb","provenance":[],"authorship_tag":"ABX9TyOt/TUQWFPtCJ++yN5cvyMi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-WEDZ53Zbu0h"},"source":["# Question 2b0\n","\n","Prepare"]},{"cell_type":"markdown","metadata":{"id":"L5pxx5f-Uskg"},"source":["# Imports and Setup"]},{"cell_type":"code","metadata":{"id":"HdVcgqJuT-r1","executionInfo":{"status":"ok","timestamp":1605122385237,"user_tz":-480,"elapsed":37897,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"0ebbdfa9-d451-4b4c-d8de-5f94b7225474","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# chuanxin\n","%cd \"../gdrive/My Drive/cz4042_assignment_2/2b\" "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/cz4042_assignment_2/2a\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYLMIZfUUu19"},"source":["import os\n","import pickle\n","import json\n","import csv\n","import re\n","# import pylab\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import tensorflow as tf\n","from tqdm.keras import TqdmCallback\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWdh-qfGWLb1"},"source":["seed = 0\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","inspect_index = 40"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcDJZphiU4ec"},"source":["# Helper functions"]},{"cell_type":"markdown","metadata":{"id":"upXJ-u4cU8gY"},"source":["### read_data_chars()\n","Used to load in the data. Returns x_train, y_train, x_test, y_test"]},{"cell_type":"code","metadata":{"id":"cBuzMdbSU3em"},"source":["def read_data_chars():\n","    x_train, y_train, x_test, y_test = [], [], [], []\n","    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n","    with open('./train_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_train.append(data)\n","            y_train.append(int(row[0]))\n","\n","    with open('./test_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_test.append(data)\n","            y_test.append(int(row[0]))\n","\n","\n","    vocab_size, char_to_ix = vocabulary(x_train+x_test)\n","    x_train = preprocess(x_train, char_to_ix, MAX_DOCUMENT_LENGTH)\n","    y_train = np.array(y_train)\n","    x_test = preprocess(x_test, char_to_ix, MAX_DOCUMENT_LENGTH)\n","    y_test = np.array(y_test)\n","\n","    x_train = tf.constant(x_train, dtype=tf.int64)\n","    y_train = tf.constant(y_train, dtype=tf.int64)\n","    x_test = tf.constant(x_test, dtype=tf.int64)\n","    y_test = tf.constant(y_test, dtype=tf.int64)\n","\n","    return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lw4xNc4tb9mh"},"source":["### vocabulary(strings)\n","Read data with [character]. Get the unique characters in this strings and the vocab size (of characters)"]},{"cell_type":"code","metadata":{"id":"2x7REe4-cBQA"},"source":["def vocabulary(strings):\n","    chars = sorted(list(set(list(''.join(strings)))))\n","    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","    vocab_size = len(chars)\n","    return vocab_size, char_to_ix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af_uf97ecZ8M"},"source":["### preprocess(strings, char_to_ix, MAX_LENGTH)\n","Clean up a string "]},{"cell_type":"code","metadata":{"id":"E1gJ4CxYcZp5"},"source":["def preprocess(strings, char_to_ix, MAX_LENGTH):\n","    data_chars = [list(d.lower()) for _, d in enumerate(strings)]\n","    for i, d in enumerate(data_chars):\n","        if len(d)>MAX_LENGTH:\n","            d = d[:MAX_LENGTH]\n","        elif len(d) < MAX_LENGTH:\n","            d += [' '] * (MAX_LENGTH - len(d))\n","            \n","    data_ids = np.zeros([len(data_chars), MAX_LENGTH], dtype=np.int64)\n","    for i in range(len(data_chars)):\n","        for j in range(MAX_LENGTH):\n","            data_ids[i, j] = char_to_ix[data_chars[i][j]]\n","    return np.array(data_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7NvMFU2KZCc"},"source":["### make_directories()\n","Used to create directories that might not have been made"]},{"cell_type":"code","metadata":{"id":"xl440MwmKY4C"},"source":["# Create folder to store histories and figures\n","def make_directories():\n","  if not os.path.exists('./histories'):\n","      os.mkdir('./histories')\n","  if not os.path.exists('./figures'):\n","      os.mkdir('./figures')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAbBYp9iKJ6R"},"source":["### history_saver(history, filename, already_json=False)\n","Used to save a history object"]},{"cell_type":"code","metadata":{"id":"Hp6cbjJhKJnS"},"source":["# filename like 'history/model_name.json'\n","def history_saver(history, model_name, already_json=False):\n","  history_json = {}\n","\n","  if already_json:\n","    history_json = history\n","  else:\n","    history = history.history\n","    for key in history.keys():\n","      history_json[key] = history[key]\n","\n","  with open('./histories/' + model_name, 'w') as file:\n","    json.dump(history_json, file)\n","\n","  print(\"History saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zCk874hLSKK"},"source":["### history_loader(filename)\n","Used to load in a json history object"]},{"cell_type":"code","metadata":{"id":"yqCAHrE4LShy"},"source":["# filename like 'history/model_name.json'\n","def history_loader(model_name):\n","  with open('./histories/'+model_name) as json_file:\n","    history = json.load(json_file)\n","  print('History loaded')\n","  \n","  return history "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmfPiQevbXu4"},"source":["### plot_3_channel_img(np_array):"]},{"cell_type":"code","metadata":{"id":"teGJdbZ5XDwc"},"source":["def plot_3_channel_img(np_array, inspect_index=None, filename=None):\n","  if inspect_index == None:\n","    inspect_index = np.random.randint(len(np_array))\n","\n","  titles = ['Original', 'Red channel', 'Green channel', 'Blue channel']\n","  cmaps = [None, plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n","\n","  fig, axes = plt.subplots(1, 4, figsize=(32,8))\n","  objs = zip(axes, (np_array[inspect_index], *np_array[inspect_index].transpose(2,0,1)), titles, cmaps)\n","\n","  if filename == None:\n","    print(\"index\", inspect_index)\n","\n","  for ax, channel, title, cmap in objs:\n","    ax.imshow(channel, cmap=cmap)\n","    ax.set_title(title)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","  \n","  if filename != None:\n","    fig.savefig(f'./figures/{filename}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_URv4UfQoGr"},"source":["### plot_loss(history_json, model_name)\n","Plot out loss graph, and also save it"]},{"cell_type":"code","metadata":{"id":"9ERB0N0cQodE"},"source":["def plot_loss(history_json, model_name):\n","  train_loss = history_json['loss']\n","  test_loss = history_json['val_loss']\n","  title = 'Model name: ' + model_name + '\\nloss against epochs'\n","\n","  plt.plot(train_loss, label='train')\n","  plt.plot(test_loss, label='test')\n","  plt.title(title)\n","  plt.ylabel('loss')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_loss.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69De_d4XQuBZ"},"source":["### plot_acc(history_json, model_name)\n","Plot out accuracy graph, and also save it"]},{"cell_type":"code","metadata":{"id":"lU9d-fF5QuW1"},"source":["def plot_acc(history_json, model_name):\n","  train_acc = history_json['accuracy']\n","  test_acc = history_json['val_accuracy']\n","  title = 'Model name: ' + model_name + '\\naccuracy against epochs'\n","\n","  plt.plot(train_acc, label='train')\n","  plt.plot(test_acc, label='test')\n","  plt.title(title)\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_accuracy.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOAkSmZ7lpON"},"source":["### plot_activations(model, model_name, x_data, x_data_name)\n","Get the intermediate activations for a single x_data (one item in x_train or x_test)"]},{"cell_type":"code","metadata":{"id":"RfcKr-54lph1"},"source":["def plot_activations(model, model_name, x_data, x_data_name):\n","  plot_3_channel_img(x_data, filename=f'{model_name}_{x_data_name}_source.png')\n","\n","  layer_outputs = [layer.output for layer in model.layers]\n","  activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n","  activations = activation_model.predict(x_data) \n","\n","  activation_layer_names = ['conv1', 'max1', 'conv2', 'max2']  \n","  activations = activations[:4]\n","\n","  images_per_row = 10\n","  \n","  for layer_name, layer_activation in zip(activation_layer_names, activations): # Displays the feature maps\n","    n_features = layer_activation.shape[-1] # Number of features in the feature map\n","    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n","    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n","    display_grid = np.zeros((size * n_cols, images_per_row * size))\n","\n","    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n","      for row in range(images_per_row):\n","        channel_image = layer_activation[0, :, :, col * images_per_row + row]\n","        channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n","        channel_image /= channel_image.std()\n","        channel_image *= 64\n","        channel_image += 128\n","        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n","        display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image # Displays the grid \n","        \n","    scale = 1. / size\n","    plt.figure(figsize=(scale * display_grid.shape[1],\n","                        scale * display_grid.shape[0]))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.axis('off')\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n","    plt.savefig(f'./figures/{model_name}_{x_data_name}_{layer_name}.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDvlc9ZbWhZl"},"source":["# 2a0"]},{"cell_type":"code","metadata":{"id":"hs572M_waP3l"},"source":["# # Training and test\n","# make_directories()\n","# x_train, y_train = load_data('data_batch_1')\n","# x_test, y_test = load_data('test_batch_trim')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVwY7mMpVEOd"},"source":["### make_model(num_ch_c1, num_ch_c2, use_dropout)\n","Creates a CNN model that has number of channels for the Conv2D layer specified, and also dropout, if needed"]},{"cell_type":"code","metadata":{"id":"z1U1trQWVDuG"},"source":["# def make_model(num_ch_c1, num_ch_c2, use_dropout):\n","#     model = tf.keras.Sequential()\n","#     model.add(tf.keras.layers.Input(shape=(32, 32, 3), name='input'))\n","#     model.add(tf.keras.layers.Conv2D(num_ch_c1, 9, activation='relu', input_shape=(None, None, 3), padding='valid', name='conv1'))\n","#     model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid', name='max1'))\n","#     model.add(tf.keras.layers.Conv2D(num_ch_c2, 5, activation='relu', input_shape=(None, None, 3), padding='valid', name='conv2'))\n","#     model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid', name='max2'))\n","#     model.add(tf.keras.layers.Flatten(name='flatten'))\n","#     if use_dropout:\n","#       model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout1'))\n","#     model.add(tf.keras.layers.Dense(300, use_bias=True, name='dense1'))\n","#     if use_dropout:\n","#       model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout2'))\n","#     model.add(tf.keras.layers.Dense(10, use_bias=True, input_shape=(300,), name='dense2'))  # Here no softmax because we have combined it with the loss    \n","    \n","#     return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Th7TzeVxauUA"},"source":["### set_model_parameters(optimizer_, model_name)\n","Select the optimizer and give the model a name that will be used to save it.\n","\n","Returns `epochs, batch_size, loss, callbacks, optimizer, model_name`"]},{"cell_type":"code","metadata":{"id":"0M7WCSjyCnAG"},"source":["# def set_model_parameters(optimizer_, model_name):\n","#   # fixed \n","#   epochs = 1000  \n","#   batch_size = 128  \n","#   learning_rate = 0.001 \n","#   loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","#   callbacks = [TqdmCallback(verbose=1)]\n","\n","\n","#   if optimizer_ == 'SGD':\n","#     optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","#   elif optimizer_ == 'SGD-momentum':\n","#     optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.1)\n","#   elif optimizer_ == 'RMSProp': \n","#     optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","#   elif optimizer_ == 'Adam': \n","#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","#   else:\n","#     raise NotImplementedError(f'You do not need to handle [{optimizer_}] in this project.')\n","\n","#   return epochs, batch_size, loss, callbacks, optimizer, model_name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_VZEwLsbBY6"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"WxLNUD8msgFh"},"source":["# num_ch_c1_list = [10,30,50,70,90]\n","# num_ch_c2_list = [20,40,60,80,100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JigygwFQGW47"},"source":["# for num_ch_c1 in num_ch_c1_list:\n","#   for num_ch_c2 in num_ch_c2_list:\n","\n","#     model = make_model(num_ch_c1, num_ch_c2, False)\n","#     epochs, batch_size, loss, callbacks, optimizer, model_name = set_model_parameters('SGD', f'q2_model_{num_ch_c1}_{num_ch_c2}')\n","\n","#     model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n","\n","#     history = model.fit(\n","#         x_train,\n","#         y_train,\n","#         batch_size=batch_size,\n","#         epochs=epochs,\n","#         callbacks=callbacks,\n","#         validation_data=(x_test, y_test))\n","\n","#     history_saver(history, model_name)\n","#     history_json = history_loader(model_name)\n","\n","#     plot_acc(history_json, model_name)\n","#     plot_loss(history_json, model_name)\n","\n","#     for i in range(2):\n","#       print(\"-\"*100)\n","#       print(\"x_test\", i)\n","#       x_data = np.expand_dims(x_test[i], axis=0) \n","#       plot_activations(model, model_name, x_data[:2], 'xtest' + str(i))\n","#       print(\"-\"*100)"],"execution_count":null,"outputs":[]}]}