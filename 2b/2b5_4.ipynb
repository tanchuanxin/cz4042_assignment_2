{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2b5_4.ipynb","provenance":[],"collapsed_sections":["VcDJZphiU4ec"],"authorship_tag":"ABX9TyP8kvFYPbPolZfDATJyzguE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-WEDZ53Zbu0h"},"source":["# Question 2b5_4\n","Compare the test accuracies and the running times of the networks implemented in parts (1) – (4).\n","\n","Experiment with adding dropout to the layers of networks in parts (1) – (4), and report the test accuracies. Compare and comment on the accuracies of the networks with/without dropout."]},{"cell_type":"markdown","metadata":{"id":"L5pxx5f-Uskg"},"source":["# Imports and Setup"]},{"cell_type":"code","metadata":{"id":"HdVcgqJuT-r1","executionInfo":{"status":"ok","timestamp":1605190700958,"user_tz":-480,"elapsed":18094,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"a265f283-a08d-4b4a-b66c-13242592356a","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# chuanxin\n","%cd \"../gdrive/My Drive/cz4042_assignment_2/2b\" "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/cz4042_assignment_2/2b\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYLMIZfUUu19","executionInfo":{"status":"ok","timestamp":1605190704405,"user_tz":-480,"elapsed":21531,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"e5f74e1b-9214-470c-8f08-aed309139eac","colab":{"base_uri":"https://localhost:8080/"}},"source":["import os\n","import time\n","import json\n","import csv\n","import re\n","import collections\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EWdh-qfGWLb1"},"source":["seed = 10\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcDJZphiU4ec"},"source":["# Helper functions"]},{"cell_type":"markdown","metadata":{"id":"upXJ-u4cU8gY"},"source":["### read_data_words()\n","Used to load in the data. Returns x_train, y_train, x_test, y_test"]},{"cell_type":"code","metadata":{"id":"cBuzMdbSU3em"},"source":["def read_data_words():\n","    x_train, y_train, x_test, y_test = [], [], [], []\n","    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n","    with open('./train_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_train.append(data)\n","            y_train.append(int(row[0]))\n","\n","    with open('./test_medium.csv', encoding='utf-8') as filex:\n","        reader = csv.reader(filex)\n","        for row in reader:\n","            data = cop.sub(\"\", row[1])\n","            x_test.append(data)\n","            y_test.append(int(row[0]))\n","\n","    word_dict = build_word_dict(x_train+x_test)\n","    x_train = preprocess(x_train, word_dict, MAX_DOCUMENT_LENGTH)\n","    y_train = np.array(y_train)\n","    x_test = preprocess(x_test, word_dict, MAX_DOCUMENT_LENGTH)\n","    y_test = np.array(y_test)\n","\n","    x_train = [x[:MAX_DOCUMENT_LENGTH] for x in x_train]\n","    x_test = [x[:MAX_DOCUMENT_LENGTH] for x in x_test]\n","    x_train = tf.constant(x_train, dtype=tf.int64)\n","    y_train = tf.constant(y_train, dtype=tf.int64)\n","    x_test = tf.constant(x_test, dtype=tf.int64)\n","    y_test = tf.constant(y_test, dtype=tf.int64)\n","\n","    vocab_size = tf.get_static_value(tf.reduce_max(x_train))\n","    vocab_size = max(vocab_size, tf.get_static_value(tf.reduce_max(x_test))) + 1\n","    return x_train, y_train, x_test, y_test, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lw4xNc4tb9mh"},"source":["### clean_str(text)\n","Cleans a text and gets right of unwanted characters"]},{"cell_type":"code","metadata":{"id":"2x7REe4-cBQA"},"source":["def clean_str(text):\n","    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    text = text.strip().lower()\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zq5FChA5DRIL"},"source":["### build_word_dict(contents)\n","Self-explanatory"]},{"cell_type":"code","metadata":{"id":"vwF0OuiKDQxX"},"source":["def build_word_dict(contents):\n","    words = list()\n","    for content in contents:\n","        for word in word_tokenize(clean_str(content)):\n","            words.append(word)\n","\n","    word_counter = collections.Counter(words).most_common()\n","    word_dict = dict()\n","    word_dict[\"<pad>\"] = 0\n","    word_dict[\"<unk>\"] = 1\n","    word_dict[\"<eos>\"] = 2\n","    for word, _ in word_counter:\n","        word_dict[word] = len(word_dict)\n","    return word_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af_uf97ecZ8M"},"source":["### preprocess(contents, word_dict, document_max_len)\n","Clean up a string "]},{"cell_type":"code","metadata":{"id":"E1gJ4CxYcZp5"},"source":["def preprocess(contents, word_dict, document_max_len):\n","    x = list(map(lambda d: word_tokenize(clean_str(d)), contents))\n","    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n","    x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n","    x = list(map(lambda d: d[:document_max_len], x))\n","    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7NvMFU2KZCc"},"source":["### make_directories()\n","Used to create directories that might not have been made"]},{"cell_type":"code","metadata":{"id":"xl440MwmKY4C"},"source":["# Create folder to store histories and figures\n","def make_directories():\n","  if not os.path.exists('./histories'):\n","    os.mkdir('./histories')\n","  if not os.path.exists('./figures'):\n","    os.mkdir('./figures')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAbBYp9iKJ6R"},"source":["### history_saver(history, filename, already_json=False)\n","Used to save a history object"]},{"cell_type":"code","metadata":{"id":"Hp6cbjJhKJnS"},"source":["# filename like 'history/model_name.json'\n","def history_saver(history, model_name, already_json=False):\n","  history_json = {}\n","\n","  if already_json:\n","    history_json = history\n","  else:\n","    history = history.history\n","    for key in history.keys():\n","      history_json[key] = history[key]\n","\n","  with open('./histories/' + model_name, 'w') as file:\n","    json.dump(history_json, file)\n","\n","  print(\"History saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-zCk874hLSKK"},"source":["### history_loader(filename)\n","Used to load in a json history object"]},{"cell_type":"code","metadata":{"id":"yqCAHrE4LShy"},"source":["# filename like 'history/model_name.json'\n","def history_loader(model_name):\n","  with open('./histories/'+model_name) as json_file:\n","    history = json.load(json_file)\n","  print('History loaded')\n","  \n","  return history "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_URv4UfQoGr"},"source":["### plot_loss(history_json, model_name)\n","Plot out loss graph, and also save it"]},{"cell_type":"code","metadata":{"id":"9ERB0N0cQodE"},"source":["def plot_loss(history_json, model_name):\n","  train_loss = history_json['loss']\n","  test_loss = history_json['test_loss']\n","  title = 'Model name: ' + model_name + '\\nloss against epochs'\n","\n","  plt.plot(train_loss, label='train')\n","  plt.plot(test_loss, label='test')\n","  plt.title(title)\n","  plt.ylabel('loss')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_loss.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69De_d4XQuBZ"},"source":["### plot_acc(history_json, model_name)\n","Plot out accuracy graph, and also save it"]},{"cell_type":"code","metadata":{"id":"lU9d-fF5QuW1"},"source":["def plot_acc(history_json, model_name):\n","  train_acc = history_json['accuracy']\n","  test_acc = history_json['test_accuracy']\n","  title = 'Model name: ' + model_name + '\\naccuracy against epochs'\n","\n","  plt.plot(train_acc, label='train')\n","  plt.plot(test_acc, label='test')\n","  plt.title(title)\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epochs')\n","  plt.legend()\n","  plt.savefig(f'./figures/{model_name}_accuracy.png')\n","  \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDvlc9ZbWhZl"},"source":["# 2b5_4 - WordRNN (dropout)"]},{"cell_type":"code","metadata":{"id":"z1U1trQWVDuG"},"source":["tf.keras.backend.set_floatx('float32')\n","class WordRNN(tf.keras.Model):\n","  def __init__(self, vocab_size, use_dropout, hidden_dims, rnn_type, num_rnn_layers):\n","    super(WordRNN, self).__init__()\n","    self.vocab_size = vocab_size\n","    self.use_dropout = use_dropout\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_DOCUMENT_LENGTH)\n","\n","    # Weight variables and RNN cell\n","    self.hidden_dims = hidden_dims\n","    self.rnn_type = rnn_type\n","    self.num_rnn_layers = num_rnn_layers\n","\n","    if self.rnn_type == 'GRU' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'GRU' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.GRUCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'SimpleRNN' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True)     \n","    elif self.rnn_type == 'SimpleRNN' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dims), unroll=True)     \n","    elif self.rnn_type == 'LSTM' and self.num_rnn_layers == 1:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True)\n","    elif self.rnn_type == 'LSTM' and self.num_rnn_layers == 2:\n","      self.rnn1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True, return_sequences=True)\n","      self.rnn2 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dims), unroll=True)\n","    else:\n","      print(\"wrong input\")  \n","\n","    self.dense = tf.keras.layers.Dense(MAX_LABEL, activation=None)\n","\n","  def call(self, x, drop_rate):\n","    # forward\n","    embedding = self.embedding(x)\n","    encoding = self.rnn1(embedding)\n","    if self.num_rnn_layers == 2:\n","      encoding = self.rnn2(encoding)\n","    if self.use_dropout:\n","      encoding = tf.nn.dropout(encoding, drop_rate)\n","    logits = self.dense(encoding)\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOf6Fd5rueTA"},"source":["# Training function\n","def train_step(model, x, label, optimizer, drop_rate):\n","  with tf.GradientTape() as tape:\n","    out = model(x, drop_rate)\n","    loss = loss_object(label, out)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","      \n","  train_loss(loss)\n","  train_accuracy(label, out)\n","\n","# Testing function\n","def test_step(model, x, label, drop_rate):\n","  out = model(x, drop_rate)\n","  t_loss = loss_object(label, out)\n","  test_loss(t_loss)\n","  test_accuracy(label, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9tl4IUM5uhlx"},"source":["def fit(model):\n","  train_acc_list, train_loss_list, test_acc_list, test_loss_list = [], [], [], []\n","  time_start = time.perf_counter()\n","\n","  for epoch in range(no_epochs):\n","    # Reset the metrics at the start of the next epoch\n","    train_accuracy.reset_states()\n","    train_loss.reset_states()\n","    test_accuracy.reset_states()\n","    test_loss.reset_states()\n","\n","    for images, labels in train_ds:\n","      train_step(model, images, labels, optimizer, 0.5)\n","\n","    for images, labels in test_ds:\n","      test_step(model, images, labels, 0)\n","\n","    \n","    train_acc_list.append(train_accuracy.result())\n","    train_loss_list.append(train_loss.result())\n","    test_acc_list.append(test_accuracy.result())\n","    test_loss_list.append(test_loss.result())\n","\n","    template = 'Epoch {}, train_accuracy: {}, train_loss: {}, test_accuracy: {}, test_loss: {}'\n","    print (template.format(epoch+1,\n","                          train_accuracy.result(),\n","                          train_loss.result(),\n","                          test_accuracy.result(),\n","                          test_loss.result()))\n","\n","  time_stop = time.perf_counter()\n","  time_taken = time_stop-time_start\n","\n","  return train_acc_list, train_loss_list, test_acc_list, test_loss_list, time_taken"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCH46KpPx1Wu"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"9lUtvHlHuUTT"},"source":["MAX_DOCUMENT_LENGTH = 100\n","HIDDEN_SIZE = 20\n","RNN_TYPE = 'GRU'\n","NUM_RNN_LAYERS = 1\n","MAX_LABEL = 15\n","EMBEDDING_SIZE = 20\n","\n","batch_size = 128\n","no_epochs = 400\n","lr = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9LTnMMI1IUd"},"source":["# Choose optimizer and loss function for training\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","# Select metrics to measure the loss and the accuracy of the model. \n","# These metrics accumulate the values over epochs and then print the overall result.\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW3ZGh2c2wyZ"},"source":["# Training and test\n","make_directories()\n","x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n","\n","# Use `tf.data` to batch and shuffle the dataset:\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEagapWE0uz_","outputId":"6867b373-71f5-4694-96ac-e7b164b7240c","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = WordRNN(vocab_size, False, HIDDEN_SIZE, RNN_TYPE, NUM_RNN_LAYERS)\n","model_name = 'q5_4_word_rnn_gru_dropout'\n","train_acc_list, train_loss_list, test_acc_list, test_loss_list, time_taken = fit(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1, train_accuracy: 0.07071428745985031, train_loss: 2.6645185947418213, test_accuracy: 0.0714285746216774, test_loss: 2.6496849060058594\n","Epoch 2, train_accuracy: 0.07446428388357162, train_loss: 2.645779609680176, test_accuracy: 0.10571428388357162, test_loss: 2.631450653076172\n","Epoch 3, train_accuracy: 0.17464286088943481, train_loss: 2.2418911457061768, test_accuracy: 0.2314285784959793, test_loss: 1.876656174659729\n","Epoch 4, train_accuracy: 0.40089285373687744, train_loss: 1.4777427911758423, test_accuracy: 0.5214285850524902, test_loss: 1.3119338750839233\n","Epoch 5, train_accuracy: 0.6875, train_loss: 0.9024738669395447, test_accuracy: 0.6942856907844543, test_loss: 0.9085351824760437\n","Epoch 6, train_accuracy: 0.8326785564422607, train_loss: 0.5009641051292419, test_accuracy: 0.7857142686843872, test_loss: 0.7675251960754395\n","Epoch 7, train_accuracy: 0.8973214030265808, train_loss: 0.2821933329105377, test_accuracy: 0.8057143092155457, test_loss: 0.6372633576393127\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YXPSuKJl5ZAt"},"source":["for metric_list in [train_acc_list, train_loss_list, test_acc_list, test_loss_list]:\n","  metric_list[:] = [x.numpy() for x in metric_list]\n","  metric_list[:] = [x.astype(float) for x in metric_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLPeXFWA10P6"},"source":["history_json = {model_name: {\n","    'accuracy': train_acc_list,\n","    'test_accuracy': test_acc_list,\n","    'loss': train_loss_list,\n","    'test_loss': test_loss_list,\n","    'time_taken': time_taken\n","}}\n","\n","history_saver(history_json, model_name, already_json=True)\n","histories_json = history_loader(model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUr2r3jh11Ak"},"source":["plot_acc(histories_json[model_name], model_name)\n","plot_loss(histories_json[model_name], model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogOPrzW4VgGb"},"source":["histories_json[model_name]['time_taken']"],"execution_count":null,"outputs":[]}]}